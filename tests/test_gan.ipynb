{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from __datasets__ import ITSDataset, DenseHazeCVPR2019Dataset\n",
    "from gan import CycleGANConfig, Generator, Discriminator, PerceptualLoss, get_cycle_gan_trainer\n",
    "from utils.checkpoints import load_checkpoint\n",
    "from utils.display import display_images\n",
    "from utils.train_test import train, test\n",
    "from utils.datasets import DomainDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:51.826115701Z",
     "start_time": "2023-09-20T11:15:49.717408685Z"
    }
   },
   "id": "956acd0c946e9181"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "config1 = CycleGANConfig(\n",
    "    \"../../commons/datasets/its/\",\n",
    "    \"HazeGan\",\n",
    "    \"v1\",\n",
    "    image_shape=(3, 64, 64),\n",
    "    latent_dim=64,\n",
    "    dropout=0.3,\n",
    "    num_epochs=1, batch_size=8,\n",
    "    lr=2e-4,\n",
    "    betas=(0.5, 0.999),\n",
    "    lambdas=(10, 0.5),\n",
    "    residuals=5,\n",
    "    blocks=(64, 128, 256, 512),\n",
    "    writer=True,\n",
    ")\n",
    "config2 = CycleGANConfig(\n",
    "    \"../../commons/datasets/dense_haze_cvpr2019/\",\n",
    "    \"HazeGan\",\n",
    "    \"v1\",\n",
    "    image_shape=(3, 128, 128),\n",
    "    latent_dim=64,\n",
    "    dropout=0.3,\n",
    "    num_epochs=1, batch_size=8,\n",
    "    lr=2e-4,\n",
    "    betas=(0.5, 0.999),\n",
    "    lambdas=(10, 0.5),\n",
    "    residuals=5,\n",
    "    blocks=(64, 128, 256, 512),\n",
    "    writer=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:51.841564740Z",
     "start_time": "2023-09-20T11:15:51.829606979Z"
    }
   },
   "id": "3360d2818ce85e56"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(2798, 55)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds1 = DomainDataset(\n",
    "    ITSDataset(config1.dataset_path, SET=\"hazy\", download=True, image_transform=config1.transforms, sub_sample=0.2),\n",
    "    ITSDataset(config1.dataset_path, SET=\"clear\", download=True, image_transform=config1.transforms, sub_sample=1)\n",
    ")\n",
    "ds2 = DomainDataset(\n",
    "    DenseHazeCVPR2019Dataset(config2.dataset_path, SET=\"hazy\", download=True, image_transform=config2.transforms, sub_sample=1),\n",
    "    DenseHazeCVPR2019Dataset(config2.dataset_path, SET=\"GT\", download=True, image_transform=config2.transforms, sub_sample=1)\n",
    ")\n",
    "len(ds1), len(ds2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:51.919355501Z",
     "start_time": "2023-09-20T11:15:51.838522422Z"
    }
   },
   "id": "b255bf8e9f03f347"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "generatorA = Generator(\n",
    "    config1.image_shape[0],\n",
    "    config1.latent_dim,\n",
    "    config1.residuals,\n",
    "    p=config1.dropout,\n",
    "    coder_len=config1.coder_len,\n",
    ").to(config1.device)\n",
    "generatorB = Generator(\n",
    "    config1.image_shape[0],\n",
    "    config1.latent_dim,\n",
    "    config1.residuals,\n",
    "    p=config1.dropout,\n",
    "    coder_len=config1.coder_len,\n",
    ").to(config1.device)\n",
    "discriminatorA = Discriminator(config1.image_shape[0], list(config1.blocks), p=config1.dropout).to(config1.device)\n",
    "discriminatorB = Discriminator(config1.image_shape[0], list(config1.blocks), p=config1.dropout).to(config1.device)\n",
    "optimizerG = optim.Adam(\n",
    "    list(generatorA.parameters()) + list(generatorB.parameters()),\n",
    "    lr=config1.lr,\n",
    "    betas=config1.betas\n",
    ")\n",
    "optimizerD = optim.Adam(\n",
    "    list(discriminatorA.parameters()) + list(discriminatorB.parameters()),\n",
    "    lr=config1.lr,\n",
    "    betas=config1.betas\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:52.112561440Z",
     "start_time": "2023-09-20T11:15:51.927342781Z"
    }
   },
   "id": "146a7274921d27d1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "perceptual_model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:35].to(config1.device)\n",
    "perceptual_loss = PerceptualLoss(perceptual_model)\n",
    "fixedA, fixedB = ds1[:4].values()\n",
    "trainer = get_cycle_gan_trainer(generatorA, generatorB, discriminatorA, discriminatorB, optimizerG, optimizerD,\n",
    "                                save_path=config1.checkpoint_path,\n",
    "                                perceptual_loss=perceptual_loss, lambda_cycle=config1.lambdas[0],\n",
    "                                lambda_identity=config1.lambdas[1],\n",
    "                                writer=config1.writer, period=100,\n",
    "                                fixedA=fixedA[\"image\"], fixedB=fixedB[\"image\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:53.791738518Z",
     "start_time": "2023-09-20T11:15:52.116458584Z"
    }
   },
   "id": "4284189ab6408161"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "step = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:15:54.083250449Z",
     "start_time": "2023-09-20T11:15:54.037204511Z"
    }
   },
   "id": "af9d70cc8fea5a22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \".pt\"\n",
    "others = load_checkpoint(\n",
    "    file_path,\n",
    "    {\"generatorA\": generatorA, \"generatorB\": generatorB, \"discriminatorA\": discriminatorA, \"discriminatorB\": discriminatorB},\n",
    "    {\"optimizerG\": optimizerG, \"optimizerD\": optimizerD},\n",
    ")\n",
    "step = others[\"step\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ba2288a732c3dc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Epoch: 0/1 | Batch 0/350|          |  0% [00:00<?, ?it/s, loss=?]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f3f62b030f4444e86ae22f817cc9190"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m step \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mds1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mne\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/utils/train_test.py:21\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(trainer, ds, ne, bs, collate_fn, step_offset)\u001B[0m\n\u001B[1;32m     19\u001B[0m prog: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTQDM\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m tqdm(dl, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: 0/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mne\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Batch\u001B[39m\u001B[38;5;124m\"\u001B[39m, postfix\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m?\u001B[39m\u001B[38;5;124m\"\u001B[39m}, bar_format\u001B[38;5;241m=\u001B[39mBAR_FORMAT)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, DATA \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(prog):\n\u001B[0;32m---> 21\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     23\u001B[0m     prog\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mne\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Batch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/gan/trainer.py:40\u001B[0m, in \u001B[0;36mget_cycle_gan_trainer.<locals>.trainer\u001B[0;34m(DATA, step)\u001B[0m\n\u001B[1;32m     38\u001B[0m realA, realB \u001B[38;5;241m=\u001B[39m DATA[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdomain_0\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m], DATA[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdomain_1\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     39\u001B[0m fakeA, fakeB \u001B[38;5;241m=\u001B[39m generatorA(realB), generatorB(realA)\n\u001B[0;32m---> 40\u001B[0m backA, backB \u001B[38;5;241m=\u001B[39m \u001B[43mgeneratorA\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfakeB\u001B[49m\u001B[43m)\u001B[49m, generatorB(fakeA)\n\u001B[1;32m     41\u001B[0m sameA, sameB \u001B[38;5;241m=\u001B[39m generatorA(realA), generatorB(realB)\n\u001B[1;32m     42\u001B[0m pred_realA, pred_realB \u001B[38;5;241m=\u001B[39m discriminatorA(realA), discriminatorB(realB)\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/gan/generator.py:85\u001B[0m, in \u001B[0;36mGenerator.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m skip_connection, up_block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mreversed\u001B[39m(skip_connections), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_blocks):\n\u001B[1;32m     84\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([x, skip_connection], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 85\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mup_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpred(x)\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/venv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/hazemaze/venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:956\u001B[0m, in \u001B[0;36mConvTranspose2d.forward\u001B[0;34m(self, input, output_size)\u001B[0m\n\u001B[1;32m    951\u001B[0m num_spatial_dims \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    952\u001B[0m output_padding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_padding(\n\u001B[1;32m    953\u001B[0m     \u001B[38;5;28minput\u001B[39m, output_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkernel_size,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    954\u001B[0m     num_spatial_dims, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[0;32m--> 956\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv_transpose2d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    957\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_padding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "step = train(\n",
    "    trainer, ds1,\n",
    "    ne=config1.num_epochs, bs=config1.batch_size,\n",
    "    step_offset=step,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:16:05.653735583Z",
     "start_time": "2023-09-20T11:15:54.264639358Z"
    }
   },
   "id": "ca99c3a43637171f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.65 s ± 268 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "_, fixedB = ds2[:24].values()\n",
    "generatorA = generatorA.eval()\n",
    "with torch.inference_mode():\n",
    "    %timeit generatorA(fixedB[\"image\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:16:53.450878165Z",
     "start_time": "2023-09-20T11:16:13.475284046Z"
    }
   },
   "id": "954830dfaed72fe4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fixedA, fixedB = ds2[:9].values()\n",
    "with torch.inference_mode():\n",
    "    grid_realA = make_grid(fixedA, nrow=1, normalize=True)\n",
    "    grid_realB = make_grid(fixedB, nrow=1, normalize=True)\n",
    "    grid_fakeA = make_grid(fakeA := generatorA(fixedB), nrow=1, normalize=True)\n",
    "    grid_fakeB = make_grid(fakeB := generatorB(fixedA), nrow=1, normalize=True)\n",
    "    grid_cycleA = make_grid(generatorA(fakeB), nrow=1, normalize=True)\n",
    "    grid_cycleB = make_grid(generatorB(fakeA), nrow=1, normalize=True)\n",
    "    grid_identityA = make_grid(identityA := generatorA(fixedA), nrow=1, normalize=True)\n",
    "    grid_identityB = make_grid(identityB := generatorB(fixedB), nrow=1, normalize=True)\n",
    "    grid_doubleA = make_grid(generatorA(identityB), nrow=1, normalize=True)\n",
    "    grid_doubleB = make_grid(generatorB(identityA), nrow=1, normalize=True)\n",
    "    gridA = make_grid(torch.stack([grid_realA, grid_fakeB, grid_doubleB, grid_cycleA, grid_identityA]), nrow=5, normalize=True)\n",
    "    gridB = make_grid(torch.stack([grid_realB, grid_fakeA, grid_doubleA, grid_cycleB, grid_identityB]), nrow=5, normalize=True)\n",
    "display_images(torch.stack([gridA, gridB]).permute(0, 2, 3, 1).cpu())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acbbe2e7f2efdd8b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_image(gridA, \"gridA.png\"), save_image(gridB, \"gridB.png\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "725e01c882dfe5d7"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "hazy = Image.open(\"../utils/img.png\")\n",
    "hazy = ToTensor()(hazy).unsqueeze(0).to(config1.device)\n",
    "hazy = Normalize(config1.mean, config1.std)(hazy)\n",
    "generatorA = generatorA.eval()\n",
    "with torch.inference_mode(): clear = generatorA(hazy)\n",
    "grid_eg = make_grid(torch.cat([hazy, clear]), nrow=2, normalize=True)\n",
    "display_images(torch.stack([grid_eg]).permute(0, 2, 3, 1).cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T13:18:18.149762695Z",
     "start_time": "2023-09-20T13:18:07.931741302Z"
    }
   },
   "id": "3c325dc75843b33b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e79d9a2a6909e4e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
